{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.book import text6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide recommendations of a next word given the previous word, I'ld like to get a list of all the words that typically follow another word and how often those words were mentioned. To do this I am going to create a dictionary whose keywords are the words and the items are named tuples. The named tuples will have a list of words that occurced after the keyword and a list the number of times it was used.\n",
    "\n",
    "For example, suppose we were only analyzing the text that read as follows.\n",
    "\n",
    "\"Hello world. Hello world. Why hello there.\" \n",
    "\n",
    "The word \"hello\" would have two lists ['world', 'there'] and [2,1] because the word \"world\" succeeds \"hello\" twice in the text while \"there\" succeeds \"hello\" once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word = collections.namedtuple(\"Word\", ['after_words','counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the spirit of python we will use Monty Python and the Holy Grail for this. Let's get a list of bigrams.\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write some functions to clean the text and omit certain text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ' '.join(text6.tokens)\n",
    "\n",
    "text[:100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns True for text we want to omit and False otherwise\n",
    "def omit_text(text):\n",
    "    \n",
    "    #Eliminate these marks for now as I don't think they will be meaningful\n",
    "    is_a_mark = text in [\"_\", \"$\", \"#\", \";\", \",\", \"]\", \n",
    "                         \"[\", \"{\", \"}\", \"\", \" \", \":\", \n",
    "                         \"!\", \"'\", \".\", '--', '?', '...']\n",
    "    \n",
    "    #You can add more conditions. In previous commits I eliminated numbers, but I decided to loosen up a bit.\n",
    "    \n",
    "    answer = True if is_a_mark else False\n",
    "    \n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omit_text(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE', '1', 'wind', 'clop', 'clop']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = text.split(\" \")\n",
    "words = [w for w in all_words if not omit_text(w)]\n",
    "\n",
    "words[:5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the       299\nI         255\nARTHUR    225\nyou       204\na         188\nof        158\nto        144\ns         141\nand       135\nOh        110\ndtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts =pd.Series(words).value_counts()\n",
    "\n",
    "word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = [b for b in zip(words[:-1], words[1:])]\n",
    "\n",
    "bigrams[:5]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "\n",
    "for word in list(set(words)):\n",
    "    all_words_after = [w[1] for w in bigrams if w[0] == word]\n",
    "    word_counts = pd.Series(all_words_after).value_counts()\n",
    "    \n",
    "    word_dict[word] = Word(after_words=word_counts.index.tolist(), counts=word_counts.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word(after_words=['Arthur', 'of', 'you', 'who'], counts=[16, 9, 1, 1])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict['King']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/monty_python_holy_grail_word_data.json\", \"w\") as file:\n",
    "    json.dump(word_dict, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
