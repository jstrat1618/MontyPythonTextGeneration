{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\nLoading text1, ..., text9 and sent1, ..., sent9\nType the name of the text or sentence to view it.\nType: 'texts()' or 'sents()' to list the materials.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: Moby Dick by Herman Melville 1851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text2: Sense and Sensibility by Jane Austen 1811\ntext3: The Book of Genesis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text4: Inaugural Address Corpus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text5: Chat Corpus\ntext6: Monty Python and the Holy Grail\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text7: Wall Street Journal\ntext8: Personals Corpus\ntext9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.book import text6\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is two-fold create data for our text recommender application and generate sentences using the text data from Monty Python and the Holy Grail.\n",
    "\n",
    "We will base this on word pairs aka bigrams. For example, the sentence \"This is a sentence\" has the following word pairs (This, is), (is, a) and (a, sentence). So for every word mentioned in the Monty Python and the Holy Grail, I'ld like to get a list of the words that followed that word and how often those words were mentioned. To do this I am going to create a dictionary whose keywords are the words and the items are named tuples. The named tuples will have a list of words that occurced after the keyword and a list the number of times it was used.\n",
    "\n",
    "For example, suppose we were only analyzing the text that read as follows.\n",
    "\n",
    "\"Hello world. Hello world. Why hello there.\" \n",
    "\n",
    "The word \"hello\" would have two lists ['world', 'there'] and [2,1] because the word \"world\" succeeds \"hello\" twice in the text while \"there\" succeeds \"hello\" once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Word = collections.namedtuple(\"Word\", ['after_words','counts'])\n",
    "WordProb = collections.namedtuple(\"Word\", ['after_words','probs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to write some functions to clean the text and omit certain text. The text has already been tokenized. However, the text has been split by punctuation.  So words like \"it's\" have been split into (it, ') and (',s). So we will join all the tokens and then clean up the text some more.",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SCENE 1 : [ wind ] [ clop clop clop ] KING ARTHUR : Whoa there ! [ clop clop clop ] SOLDIER # 1 : Ha'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ' '.join(text6.tokens)\n",
    "#This should take care of the apostrophes\n",
    "text = text.replace(\" ' \", \"'\")\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"it's\" in text #Test this out by commengint out the replace method in the block above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I am going to write a function that returns True if the text is a text that I want to omit. As of right now I am only elminating some punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns True for text we want to omit and False otherwise\n",
    "def omit_text(text):\n",
    "    \n",
    "    #Eliminate these marks for now as I don't think they will be meaningful\n",
    "    is_a_mark = text in [\"_\", \"$\", \"#\", \";\", \",\", \"]\", \n",
    "                         \"[\", \"{\", \"}\", \"\", \" \", \":\", \n",
    "                         \"!\", \"'\", \".\", '--', '?', '...']\n",
    "    \n",
    "    #You can add more conditions. In previous commits I eliminated numbers, but I decided to loosen up a bit.\n",
    "    \n",
    "    answer = True if is_a_mark else False\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omit_text(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SCENE', '1', 'wind', 'clop', 'clop']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = text.split(\" \")\n",
    "words = [w for w in all_words if not omit_text(w)]\n",
    "\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "the       298\nARTHUR    225\nI         203\nyou       188\na         187\nof        158\nto        142\nand       133\nOh        110\nis        100\ndtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts =pd.Series(words).value_counts()\n",
    "\n",
    "word_counts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('SCENE', '1'),\n ('1', 'wind'),\n ('wind', 'clop'),\n ('clop', 'clop'),\n ('clop', 'clop')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = [b for b in zip(words[:-1], words[1:])]\n",
    "\n",
    "bigrams[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "\n",
    "for word in list(set(words)):\n",
    "    all_words_after = [w[1] for w in bigrams if w[0] == word]\n",
    "    word_counts = pd.Series(all_words_after).value_counts()\n",
    "    \n",
    "    word_dict[word] = Word(after_words=word_counts.index.tolist(), counts=word_counts.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Word(after_words=['Arthur', 'of', \"Arthur's\", 'you', 'who'], counts=[15, 9, 1, 1, 1])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_dict[\"King\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to save the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/monty_python_holy_grail_word_data.json\", \"w\") as file:\n",
    "    json.dump(word_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences\n",
    "Next I'd like to know how long each sentence is usually in words. To do this I will write a simple function to count number of words in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_words(text_dum):\n",
    "    \n",
    "    word_list = text_dum.split(\" \")\n",
    "    \n",
    "    return len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17 11  4 19 15]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=1882, minmax=(1, 116), mean=8.5712008501594052, variance=56.189770912322501, skewness=3.7646419885765847, kurtosis=30.017681161161036)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "sent_length = np.array([num_words(sent) for sent in sentences])\n",
    "\n",
    "print(sent_length[:5])\n",
    "\n",
    "stats.describe(sent_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x222c3365fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sent_length, bins='auto',)\n",
    "plt.title(\"Histogram of Sentence Length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the sentence length varies quite widely. The mean is about 9 and the variance is about 60. Next I would like to know which words commonly start a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ARTHUR       0.103408\nLAUNCELOT    0.035817\nGALAHAD      0.035817\nBEDEVERE     0.026574\nI            0.025997\nVILLAGER     0.025419\nFATHER       0.024841\nROBIN        0.019064\nGUARD        0.016753\nKNIGHTS      0.016753\ndtype: float64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_words = [x.split(\" \")[0] for x in sentences if not omit_text(x.split(\" \")[0])]\n",
    "\n",
    "starts = pd.Series(start_words).value_counts()/sum(pd.Series(start_words).value_counts())\n",
    "\n",
    "starts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I would like to create a function that randomly picks a start word according to these probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_starts = starts.tolist()\n",
    "\n",
    "sent_starts = starts.index.tolist()\n",
    "\n",
    "def start_sentence(sent_starters=sent_starts, prob=prob_starts):\n",
    "    return np.random.choice(sent_starters, p=prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FATHER'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_sentence(sent_starts, prob_starts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I would like to write  a function that picks the subsequent word at random according to the probabilities derieved from the word counts in word_dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale1(x):\n",
    "    x = np.array(x)\n",
    "    \n",
    "    return list(x/x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_dict = {}\n",
    "\n",
    "for key,val in word_dict.items():\n",
    "    after_words, counts = word_dict[key]\n",
    "    probs = scale1(counts)\n",
    "    \n",
    "    prob_dict[key] = WordProb(after_words=after_words, probs=probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's write a function that generates the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(last_word, word_data):\n",
    "    word = word_data[last_word]\n",
    "    \n",
    "    return np.random.choice(word.after_words, p=word.probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_word('ARTHUR', prob_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(num_words):\n",
    "    \n",
    "    last_word = start_sentence()\n",
    "    \n",
    "    sent = last_word\n",
    "    \n",
    "    sent_length = len(sent.split(' '))\n",
    "    \n",
    "    while sent_length <= num_words:\n",
    "        last_word = next_word(last_word, prob_dict)\n",
    "        sent += \" \" + last_word\n",
    "        sent_length = len(sent.split(' '))\n",
    "        \n",
    "    sent = sent + \".\"\n",
    "    \n",
    "    return sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Supreme executive power just'cause some lovely acting in.\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, I will generate the length of sentence randomly from a poisson distribution despite the fact that given our histrogram and summary statistics are not consistent with the poisson distribution because the mean and variance are far apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARTHUR You tit I shall snuff it GUARD Of course not afraid of.',\n 'ARTHUR If you have a little bit daft to.',\n 'GUARD 2 Burn BEDEVERE My God dramatic chord ARTHUR.',\n \"OLD MAN Seek you Oh yes We'll stay.\",\n 'ARTHUR Brave brave and I.']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = np.random.poisson(lam=9, size=100)\n",
    "\n",
    "sim_text = [generate_sentence(num) for num in nums]\n",
    "\n",
    "sim_text[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'sentence':sim_text}).to_csv(\"simulated_sentences.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
